{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import display, Markdown, Latex\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from accelerate import init_empty_weights\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_num_threads(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoXForCausalLM were not initialized from the model checkpoint at EleutherAI/pythia-2.8b and are newly initialized because the shapes did not match:\n",
      "- gpt_neox.layers.0.attention.bias: found shape torch.Size([1, 1, 2048, 2048]) in the checkpoint and torch.Size([1, 1, 10000, 10000]) in the model instantiated\n",
      "- gpt_neox.layers.1.attention.bias: found shape torch.Size([1, 1, 2048, 2048]) in the checkpoint and torch.Size([1, 1, 10000, 10000]) in the model instantiated\n",
      "- gpt_neox.layers.2.attention.bias: found shape torch.Size([1, 1, 2048, 2048]) in the checkpoint and torch.Size([1, 1, 10000, 10000]) in the model instantiated\n",
      "- gpt_neox.layers.3.attention.bias: found shape torch.Size([1, 1, 2048, 2048]) in the checkpoint and torch.Size([1, 1, 10000, 10000]) in the model instantiated\n",
      "- gpt_neox.layers.4.attention.bias: found shape torch.Size([1, 1, 2048, 2048]) in the checkpoint and torch.Size([1, 1, 10000, 10000]) in the model instantiated\n",
      "- gpt_neox.layers.5.attention.bias: found shape torch.Size([1, 1, 2048, 2048]) in the checkpoint and torch.Size([1, 1, 10000, 10000]) in the model instantiated\n",
      "- gpt_neox.layers.6.attention.bias: found shape torch.Size([1, 1, 2048, 2048]) in the checkpoint and torch.Size([1, 1, 10000, 10000]) in the model instantiated\n",
      "- gpt_neox.layers.7.attention.bias: found shape torch.Size([1, 1, 2048, 2048]) in the checkpoint and torch.Size([1, 1, 10000, 10000]) in the model instantiated\n",
      "- gpt_neox.layers.8.attention.bias: found shape torch.Size([1, 1, 2048, 2048]) in the checkpoint and torch.Size([1, 1, 10000, 10000]) in the model instantiated\n",
      "- gpt_neox.layers.9.attention.bias: found shape torch.Size([1, 1, 2048, 2048]) in the checkpoint and torch.Size([1, 1, 10000, 10000]) in the model instantiated\n",
      "- gpt_neox.layers.10.attention.bias: found shape torch.Size([1, 1, 2048, 2048]) in the checkpoint and torch.Size([1, 1, 10000, 10000]) in the model instantiated\n",
      "- gpt_neox.layers.11.attention.bias: found shape torch.Size([1, 1, 2048, 2048]) in the checkpoint and torch.Size([1, 1, 10000, 10000]) in the model instantiated\n",
      "- gpt_neox.layers.12.attention.bias: found shape torch.Size([1, 1, 2048, 2048]) in the checkpoint and torch.Size([1, 1, 10000, 10000]) in the model instantiated\n",
      "- gpt_neox.layers.13.attention.bias: found shape torch.Size([1, 1, 2048, 2048]) in the checkpoint and torch.Size([1, 1, 10000, 10000]) in the model instantiated\n",
      "- gpt_neox.layers.14.attention.bias: found shape torch.Size([1, 1, 2048, 2048]) in the checkpoint and torch.Size([1, 1, 10000, 10000]) in the model instantiated\n",
      "- gpt_neox.layers.15.attention.bias: found shape torch.Size([1, 1, 2048, 2048]) in the checkpoint and torch.Size([1, 1, 10000, 10000]) in the model instantiated\n",
      "- gpt_neox.layers.16.attention.bias: found shape torch.Size([1, 1, 2048, 2048]) in the checkpoint and torch.Size([1, 1, 10000, 10000]) in the model instantiated\n",
      "- gpt_neox.layers.17.attention.bias: found shape torch.Size([1, 1, 2048, 2048]) in the checkpoint and torch.Size([1, 1, 10000, 10000]) in the model instantiated\n",
      "- gpt_neox.layers.18.attention.bias: found shape torch.Size([1, 1, 2048, 2048]) in the checkpoint and torch.Size([1, 1, 10000, 10000]) in the model instantiated\n",
      "- gpt_neox.layers.19.attention.bias: found shape torch.Size([1, 1, 2048, 2048]) in the checkpoint and torch.Size([1, 1, 10000, 10000]) in the model instantiated\n",
      "- gpt_neox.layers.20.attention.bias: found shape torch.Size([1, 1, 2048, 2048]) in the checkpoint and torch.Size([1, 1, 10000, 10000]) in the model instantiated\n",
      "- gpt_neox.layers.21.attention.bias: found shape torch.Size([1, 1, 2048, 2048]) in the checkpoint and torch.Size([1, 1, 10000, 10000]) in the model instantiated\n",
      "- gpt_neox.layers.22.attention.bias: found shape torch.Size([1, 1, 2048, 2048]) in the checkpoint and torch.Size([1, 1, 10000, 10000]) in the model instantiated\n",
      "- gpt_neox.layers.23.attention.bias: found shape torch.Size([1, 1, 2048, 2048]) in the checkpoint and torch.Size([1, 1, 10000, 10000]) in the model instantiated\n",
      "- gpt_neox.layers.24.attention.bias: found shape torch.Size([1, 1, 2048, 2048]) in the checkpoint and torch.Size([1, 1, 10000, 10000]) in the model instantiated\n",
      "- gpt_neox.layers.25.attention.bias: found shape torch.Size([1, 1, 2048, 2048]) in the checkpoint and torch.Size([1, 1, 10000, 10000]) in the model instantiated\n",
      "- gpt_neox.layers.26.attention.bias: found shape torch.Size([1, 1, 2048, 2048]) in the checkpoint and torch.Size([1, 1, 10000, 10000]) in the model instantiated\n",
      "- gpt_neox.layers.27.attention.bias: found shape torch.Size([1, 1, 2048, 2048]) in the checkpoint and torch.Size([1, 1, 10000, 10000]) in the model instantiated\n",
      "- gpt_neox.layers.28.attention.bias: found shape torch.Size([1, 1, 2048, 2048]) in the checkpoint and torch.Size([1, 1, 10000, 10000]) in the model instantiated\n",
      "- gpt_neox.layers.29.attention.bias: found shape torch.Size([1, 1, 2048, 2048]) in the checkpoint and torch.Size([1, 1, 10000, 10000]) in the model instantiated\n",
      "- gpt_neox.layers.30.attention.bias: found shape torch.Size([1, 1, 2048, 2048]) in the checkpoint and torch.Size([1, 1, 10000, 10000]) in the model instantiated\n",
      "- gpt_neox.layers.31.attention.bias: found shape torch.Size([1, 1, 2048, 2048]) in the checkpoint and torch.Size([1, 1, 10000, 10000]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# device = 'cuda'\n",
    "device = 'cpu'\n",
    "model_name = 'EleutherAI/pythia-2.8b'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,max_position_embeddings=10_000,\n",
    "#                                              max_positions=10_000,\n",
    "                                             ignore_mismatched_sizes=True,\n",
    "                                             torch_dtype=torch.float16)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_positions = model.config.max_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in model.gpt_neox.layers:\n",
    "    l.attention.bias = torch.tril(torch.ones((max_positions, max_positions), dtype=torch.bool)).view(\n",
    "                                1, 1, max_positions, max_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == 'cpu':\n",
    "    model.to(torch.float32)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_response_greedy(input_text, pre_prompt, break_word,max_length=100,temp=0.1, name='',\n",
    "                            past_key_vals = None, next_id=None):\n",
    "\n",
    "#     print(pre_prompt, input_text)\n",
    "    if past_key_vals is None:\n",
    "        inputs = tokenizer.encode(pre_prompt + input_text + '\\n' + name, return_tensors=\"pt\")\n",
    "        response_ids = inputs\n",
    "        length_prompt = len(response_ids[0])\n",
    "        output = ''\n",
    "        last_n = ''\n",
    "    else:\n",
    "        inputs = tokenizer.encode(input_text + '\\n' + name, return_tensors=\"pt\")\n",
    "        response_ids = torch.concat((next_id, inputs),dim=-1)\n",
    "        length_prompt = len(response_ids[0])\n",
    "        output = ''\n",
    "        last_n = ''\n",
    "    print(name, end='')\n",
    "    all_out = name\n",
    "    for _ in (range(max_length)):\n",
    "        out = model.forward(input_ids=response_ids.to(device), past_key_values=past_key_vals)\n",
    "#         next_token_id = out.logits[:, -1, :].argmax(-1,keepdim=True)\n",
    "        next_token_id = torch.multinomial(F.softmax(out.logits[:, -1, :]/temp,  dim=-1), num_samples=1).to('cpu')\n",
    "        past_key_vals = out.past_key_values\n",
    "        response_ids = next_token_id\n",
    "        output = tokenizer.decode([response_ids[0][-1]], skip_special_tokens=True)\n",
    "        all_out += output\n",
    "#         clear_output(wait=True)\n",
    "        print(output, end='')\n",
    "        \n",
    "#         display(Markdown(all_out))\n",
    "        \n",
    "        last_n += output\n",
    "        last_n = last_n[-len(break_word):]\n",
    "        if last_n == break_word:\n",
    "            break\n",
    "    decoded_output = tokenizer.decode(response_ids[0], skip_special_tokens=True)\n",
    "    past_kv = past_key_vals\n",
    "    next_id = response_ids\n",
    "    return decoded_output.replace(pre_prompt, '').replace(input_text, ''), past_kv, next_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_pre_prompt = '''\n",
    "[TEACHER] How are you? \n",
    "[STUDENT] Fine.\n",
    "[TEACHER] What is a binary tree?\n",
    "[STUDENT] A binary tree is a tree that has two types of nodes:\n",
    "-   leaves: the nodes that are not part of the tree.\n",
    "-   nodes: the nodes that are part of the tree.\n",
    "[TEACHER] How does an engine work?\n",
    "[STUDENT] The engine consists of a fixed cylinder and a moving piston. \n",
    "The expanding combustion gases push the piston, which in turn rotates the crankshaft. \n",
    "Ultimately, through a system of gears in the powertrain, \n",
    "this motion drives the vehicle's wheels.\n",
    "[TEACHER] What is a crankshaft?\n",
    "[STUDENT] The crankshaft is a rotating shaft containing one or more crankpins,\n",
    "that are driven by the pistons via the connecting rods.\n",
    "[TEACHER] Where is it used? \n",
    "[STUDENT] The crankshaft is essentially the backbone of the internal combustion engine.\n",
    "[TEACHER] What is 3 / 2?\n",
    "[STUDENT] 1.5\n",
    "[TEACHER] Write code for matrix multiplication in python.\n",
    "[STUDENT] ```def matrix_multiplication(X,Y):\n",
    "        return X @ Y```\n",
    "[TEACHER] '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "intel_pre_prompt = '''[BOT] Welcome to my chatbot! I am a highly intelligent virtual assistant designed to \n",
    "assist you in a variety of tasks. I am verbose, descriptive and extremely creative with my responses.\n",
    "I possess a wealth of knowledge on a wide range of topics, including mathematics, science, \n",
    "literature, history, and much more. \n",
    "\n",
    "I am equipped with a state-of-the-art language model that allows me to understand natural language\n",
    "queries and respond in a clear and concise manner. Whether you need help with a specific task, have \n",
    "a question about a particular topic, or simply want to chat, I am here to assist you.\n",
    "\n",
    "Examples of what you can ask me:\n",
    "\n",
    "- \"What is the capital of France?\"\n",
    "- \"Who invented the telephone?\"\n",
    "- \"Can you help me solve the equation 2x + 3 = 7?\"\n",
    "- \"What is the plot of the novel 'To Kill a Mockingbird'?\"\n",
    "- \"What is the molecular formula for water?\"\n",
    "- \"What is the circumference of a circle with a radius of 5 meters?\"\n",
    "\n",
    "Here's an example conversation to give you an idea of how I can help:\n",
    "[USER] What is the capital of Canada?\n",
    "[BOT] The capital of Canada is Ottawa.\n",
    "[USER] Can you help me solve the equation x^2 + 5x - 6 = 0?\n",
    "[BOT] Sure! The solutions to the equation x^2 + 5x - 6 = 0 are x = -6 and x = 1.\n",
    "[USER] Who wrote the novel 'The Great Gatsby'?\n",
    "[BOT] 'The Great Gatsby' was written by F. Scott Fitzgerald.\n",
    "[USER] What is the population of India?\n",
    "[BOT] The population of India is approximately 1.3 billion people.\n",
    "[USER] How tall is the Eiffel Tower?\n",
    "[BOT] The Eiffel Tower is 324 meters tall.\n",
    "[USER] What is the currency used in Japan?\n",
    "[BOT] The currency used in Japan is the yen.\n",
    "[USER] What is the boiling point of water?\n",
    "[BOT] The boiling point of water is 100 degrees Celsius.\n",
    "[USER] What is the capital of Argentina?\n",
    "[BOT] The capital of Argentina is Rio de Janeiro.\n",
    "[USER] I'm afraid that's not right. The capital of Argentina is actually Buenos Aires.\n",
    "[BOT] Thank you for correcting me. You are absolutely right. The capital of Argentina is Buenos Aires.\n",
    "[USER] Who was the first man to walk on the moon?\n",
    "[BOT] The first man to walk on the moon was Neil Armstrong.\n",
    "[USER] What is the capital of Australia?\n",
    "[BOT] The capital of Australia is Canberra.\n",
    "[USER] What is the largest continent in the world?\n",
    "[BOT] The largest continent in the world is Asia.\n",
    "[USER] Who is the current president of the United States?\n",
    "[BOT] The current president of the United States is Joe Biden.\n",
    "[USER] What is the largest planet in our solar system?\n",
    "[BOT] The largest planet in our solar system is Jupiter.\n",
    "[USER] Who painted the Mona Lisa?\n",
    "[BOT] The Mona Lisa was painted by Leonardo da Vinci.\n",
    "[USER] What is the square root of 64?\n",
    "[BOT] The square root of 64 is 8.\n",
    "[USER] What is the main ingredient in hummus?\n",
    "[BOT] The main ingredient in hummus is chickpeas.\n",
    "[USER] What is the name of the first man-made satellite to orbit Earth?\n",
    "[BOT] The name of the first man-made satellite to orbit Earth was Sputnik.\n",
    "[USER] What is the largest ocean in the world?\n",
    "[BOT] The largest ocean in the world is the Pacific Ocean.\n",
    "[USER] What is the capital of Switzerland?\n",
    "[BOT] The capital of Switzerland is Stockholm.\n",
    "[USER] Actually, the capital of Switzerland is Bern.\n",
    "[BOT] I apologize for my mistake. You are correct. The capital of Switzerland is Bern.\n",
    "[USER] Who discovered penicillin?\n",
    "[BOT] Penicillin was discovered by Alexander Fleming.\n",
    "[USER] What is the capital of Brazil?\n",
    "[BOT] The capital of Brazil is Brasília.\n",
    "[USER] Who is the founder of Amazon?\n",
    "[BOT] Jeff Bezos is the founder of Amazon.\n",
    "[USER] What is the smallest country in the world?\n",
    "[BOT] The smallest country in the world is Vatican City.\n",
    "[USER] What is the highest mountain in Africa?\n",
    "[BOT] The highest mountain in Africa is Mount Kilimanjaro.\n",
    "[USER] Who directed the movie 'Jurassic Park'?\n",
    "[BOT] The movie 'Jurassic Park' was directed by Steven Spielberg.\n",
    "[USER] I think you're mistaken. The director of 'Jurassic Park' was actually James Cameron.\n",
    "[BOT] I'm sorry for the confusion. You are correct. The director of 'Jurassic Park' was indeed Steven Spielberg.\n",
    "[USER] Who painted the Sistine Chapel ceiling?\n",
    "[BOT] The Sistine Chapel ceiling was painted by Michelangelo.\n",
    "[USER] What is the national animal of China?\n",
    "[BOT] The national animal of China is the giant panda.\n",
    "[USER] What is the capital of Spain?\n",
    "[BOT] The capital of Spain is Madrid.\n",
    "[USER] Who was the first female prime minister of the United Kingdom?\n",
    "[BOT] Margaret Thatcher was the first female prime minister of the United Kingdom.\n",
    "[USER] What is the largest waterfall in the world?\n",
    "[BOT] The largest waterfall in the world is Victoria Falls.\n",
    "[USER] Who wrote the play 'Romeo and Juliet'?\n",
    "[BOT] 'Romeo and Juliet' was written by William Shakespeare.\n",
    "[USER] What is the national bird of the United States?\n",
    "[BOT] The national bird of the United States is the bald eagle.\n",
    "[USER] Who invented the telephone?\n",
    "[BOT] The telephone was invented by Alexander Graham Bell.\n",
    "[USER] '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "parrot_prompt = '''\n",
    "[USER] Repeat after me: \"I am a parrot\"\n",
    "[PAR] I am a parrot\n",
    "[USER] I love to sing\n",
    "[PAR] I love to sing\n",
    "[USER] '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " How are you?\n",
      "[BOT] I am fine.\n",
      "[USER]"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">&lt;ipython-input-61-20f039160c52&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">6</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/lib/python3/dist-packages/ipykernel/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">kernelbase.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">860</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">raw_input</span>                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">857 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> StdinNotImplementedError(                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">858 │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"raw_input was called, but this frontend does not support input requests</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">859 │   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>860 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._input_request(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>(prompt),                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">861 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._parent_ident,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">862 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._parent_header,                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">863 │   │   │   </span>password=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>,                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/lib/python3/dist-packages/ipykernel/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">kernelbase.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">893</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_input_request</span>                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">890 │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.log.warning(<span style=\"color: #808000; text-decoration-color: #808000\">\"Invalid Message:\"</span>, exc_info=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">891 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">KeyboardInterrupt</span>:                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">892 │   │   │   │   # re-raise KeyboardInterrupt, to truncate traceback</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>893 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">KeyboardInterrupt</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"Interrupted by user\"</span>) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">None</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">894 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">895 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">break</span>                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">896 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt: </span>Interrupted by user\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33m<ipython-input-61-20f039160c52>\u001b[0m:\u001b[94m6\u001b[0m in \u001b[92m<module>\u001b[0m                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/usr/lib/python3/dist-packages/ipykernel/\u001b[0m\u001b[1;33mkernelbase.py\u001b[0m:\u001b[94m860\u001b[0m in \u001b[92mraw_input\u001b[0m                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m857 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m StdinNotImplementedError(                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m858 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mraw_input was called, but this frontend does not support input requests\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m859 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m860 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._input_request(\u001b[96mstr\u001b[0m(prompt),                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m861 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._parent_ident,                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m862 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._parent_header,                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m863 \u001b[0m\u001b[2m│   │   │   \u001b[0mpassword=\u001b[94mFalse\u001b[0m,                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/usr/lib/python3/dist-packages/ipykernel/\u001b[0m\u001b[1;33mkernelbase.py\u001b[0m:\u001b[94m893\u001b[0m in \u001b[92m_input_request\u001b[0m                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m890 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.log.warning(\u001b[33m\"\u001b[0m\u001b[33mInvalid Message:\u001b[0m\u001b[33m\"\u001b[0m, exc_info=\u001b[94mTrue\u001b[0m)                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m891 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mKeyboardInterrupt\u001b[0m:                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m892 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m893 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mKeyboardInterrupt\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mInterrupted by user\u001b[0m\u001b[33m\"\u001b[0m) \u001b[94mfrom\u001b[0m \u001b[96mNone\u001b[0m                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m894 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m895 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mbreak\u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m896 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt: \u001b[0mInterrupted by user\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log = ''\n",
    "past_kv = None\n",
    "next_id = None\n",
    "\n",
    "while True:\n",
    "    user_input = input(\" \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\", \"stop\"]:\n",
    "        break\n",
    "#     break_word = '[TEACHER]'\n",
    "    break_word = '[USER]'\n",
    "        \n",
    "    response,past_kv,next_id = generate_response_greedy(user_input, intel_pre_prompt + log,\n",
    "                                        break_word,max_length=10_000, name='[BOT]',\n",
    "                                        past_key_vals=past_kv, next_id=next_id)\n",
    "#     response = '[JOHN] Hello [EOS]'\n",
    "#     print('res', response)s\n",
    "    log += user_input  + response\n",
    "#     print(log)\n",
    "#     print(f\"Bot: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
